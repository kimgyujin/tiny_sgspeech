{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sgspeech.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uycv4pvyWfcN",
        "outputId": "ca50d9f0-64a4-40a0-9541-1230e589c26f"
      },
      "source": [
        "!git clone https://github.com/indra622/tiny_sgspeech"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'tiny_sgspeech' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyVxqdP0Wotu",
        "outputId": "bb3962c6-1d8d-4e5b-94cb-e1612fbff306"
      },
      "source": [
        "!wget https://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
        "!wget https://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-28 06:32:08--  https://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6387309499 (5.9G) [application/x-gzip]\n",
            "Saving to: ‘train-clean-100.tar.gz.1’\n",
            "\n",
            "train-clean-100.tar  25%[====>               ]   1.49G  19.8MB/s    eta 3m 56s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gs1Lv7-XZmu"
      },
      "source": [
        "!tar -xvzf train-clean-100.tar.gz && rm train-clean-100.tar.gz\n",
        "!tar -xvzf test-clean.tar.gz && rm test-clean.tar.gz\n",
        "!tar -xvzf dev-clean.tar.gz && rm dev-clean.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvVngzpAkKpk"
      },
      "source": [
        "!python tiny_sgspeech/create_librispeech_trans.py --dir /content/LibriSpeech/train-clean-100 /content/LibriSpeech/train-clean-100/transcripts.tsv\n",
        "!python tiny_sgspeech/create_librispeech_trans.py --dir /content/LibriSpeech/dev-clean /content/LibriSpeech/dev-clean/transcripts.tsv\n",
        "!python tiny_sgspeech/create_librispeech_trans.py --dir /content/LibriSpeech/test-clean /content/LibriSpeech/test-clean/transcripts.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzqhWnnojX0j"
      },
      "source": [
        "!cd tiny_sgspeech && pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3HGKxXSt9Xo"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import argparse\n",
        "from tiny_sgspeech.sgspeech.utils import setup_environment, setup_strategy\n",
        "\n",
        "setup_environment()\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "strategy = setup_strategy([0])\n",
        "\n",
        "from tiny_sgspeech.sgspeech.configs.config import Config\n",
        "from tiny_sgspeech.sgspeech.datasets.speech_dataset import SpeechSliceDataset\n",
        "from tiny_sgspeech.sgspeech.featurizers.speech_featurizer import NumpySpeechFeaturizer\n",
        "from tiny_sgspeech.sgspeech.featurizers.text_featurizer import CharFeaturizer\n",
        "from tiny_sgspeech.sgspeech.runners.transducer_runners import TransducerTrainer\n",
        "from tiny_sgspeech.sgspeech.models.conformer import Conformer\n",
        "from tiny_sgspeech.sgspeech.optimizers.schedules import TransformerSchedule\n",
        "\n",
        "config = Config('/content/tiny_sgspeech/config.yml')\n",
        "speech_featurizer = NumpySpeechFeaturizer(config.speech_config)\n",
        "text_featurizer = CharFeaturizer(config.decoder_config)\n",
        "\n",
        "train_dataset = SpeechSliceDataset(\n",
        "    speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
        "    **vars(config.learning_config.train_dataset_config)\n",
        ")\n",
        "eval_dataset = SpeechSliceDataset(\n",
        "    speech_featurizer=speech_featurizer, text_featurizer=text_featurizer,\n",
        "    **vars(config.learning_config.eval_dataset_config)\n",
        ")\n",
        "\n",
        "conformer_trainer = TransducerTrainer(\n",
        "    config=config.learning_config.running_config,\n",
        "    text_featurizer=text_featurizer, strategy=strategy\n",
        ")\n",
        "\n",
        "with conformer_trainer.strategy.scope():\n",
        "    # build model\n",
        "    conformer = Conformer(**config.model_config, vocabulary_size=text_featurizer.num_classes)\n",
        "    conformer._build(speech_featurizer.shape)\n",
        "    conformer.summary(line_length=120)\n",
        "\n",
        "    optimizer_config = config.learning_config.optimizer_config\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        TransformerSchedule(\n",
        "            d_model=conformer.dmodel,\n",
        "            warmup_steps=optimizer_config[\"warmup_steps\"],\n",
        "            max_lr=(0.05 / math.sqrt(conformer.dmodel))\n",
        "        ),\n",
        "        beta_1=optimizer_config[\"beta1\"],\n",
        "        beta_2=optimizer_config[\"beta2\"],\n",
        "        epsilon=optimizer_config[\"epsilon\"]\n",
        "    )\n",
        "\n",
        "conformer_trainer.compile(model=conformer, optimizer=optimizer,\n",
        "                          max_to_keep=10)\n",
        "\n",
        "conformer_trainer.fit(train_dataset, eval_dataset, train_bs=4, eval_bs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rNWOU8mmabl"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/conformer/tensorboard'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg6T1-FuTuWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}